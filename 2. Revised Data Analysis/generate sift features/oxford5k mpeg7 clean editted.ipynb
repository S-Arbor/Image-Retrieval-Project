{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oxford5k + MPEG7 Image Retrieval:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean version of all functions used to compute the metrics/Image retrieval of either MPEG7 or Oxford5k dataset\n",
    "\n",
    "At the bottom of the page is the functions to run them, and instructions on inputted variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display\n",
    "import time\n",
    "import pandas\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(dir):\n",
    "    \"\"\"For a directory return (colour images, gray images, image names)\"\"\"\n",
    "\n",
    "    image_paths = []\n",
    "    image_names = []\n",
    "\n",
    "    # save path to image and save class names as numbers (train)\n",
    "    for data_path in glob.glob(dir + '/*'):\n",
    "        name = data_path.split('/')[-1].split(\"-\")[0]\n",
    "        image_names.append(name) \n",
    "        image_paths.append(data_path)\n",
    "    \n",
    "    images_colour = [cv2.imread(img_path) for img_path in image_paths]\n",
    "    images_gray = [cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in images_colour]\n",
    "\n",
    "    print(\"Loaded {} images from {}\".format(len(images_colour), dir))\n",
    "    return images_gray, image_names\n",
    "\n",
    "def load_train_data(images_path, gt_path, test_img_paths):\n",
    "    train_names = []\n",
    "    train_images_path = []\n",
    "    train_gray_images = []\n",
    "    train_colour_images = []\n",
    "\n",
    "    all_image_names = []\n",
    "\n",
    "    for filename in sorted(os.listdir(gt_path)):\n",
    "        if filename.endswith(\"good.txt\") or filename.endswith(\"ok.txt\"):\n",
    "\n",
    "            # Saving filenames\n",
    "            tmp = filename.split(\".\")[0].split(\"_\")\n",
    "            if len(tmp) == 4:\n",
    "                name = tmp[0]+\"_\"+tmp[1]\n",
    "            elif len(tmp) == 3:\n",
    "                name = tmp[0]\n",
    "\n",
    "            # Saving image paths\n",
    "            with open(os.path.join(gt_path, filename), \"r\") as f:\n",
    "                line = f.readlines()\n",
    "                for i in range(len(line)):\n",
    "                    line[i] = line[i][:-1]\n",
    "                    if line[i] not in all_image_names:\n",
    "                        if \"oxc1_\"+str(line[i]) not in test_img_paths:\n",
    "                            # Append this many names\n",
    "                            train_names.append(name)\n",
    "                            train_images_path.append(line[i])\n",
    "                            all_image_names.append(line[i])\n",
    "\n",
    "    for path in train_images_path:\n",
    "        image = cv2.imread(os.path.join(images_path, path) + \".jpg\")\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        train_gray_images.append(gray_image)\n",
    "        train_colour_images.append(image)\n",
    "    \n",
    "    print(\"Loaded in {} Images!\".format(len(train_gray_images)))\n",
    "    \n",
    "    return train_gray_images, train_colour_images, train_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basedir = \"/home/sean/Code/Pawsey/3. Data/Revised and Sorted/\"\n",
    "\n",
    "# image_data = {\"roxford5k\" : {\"easy\" : {}, \"hard\" : {}, \"query\" : {}},\n",
    "#               \"rparis6k\" : {\"easy\" : {}, \"hard\" : {}, \"query\" : {}}}\n",
    "\n",
    "# for dataset_name in image_data:\n",
    "#     for difficulty in image_data[dataset_name]:\n",
    "#         images_colour, images_gray, image_names = load_images(basedir + dataset_name + \"/\" + difficulty)\n",
    "#         image_data[dataset_name][difficulty][\"images colour\"] = images_colour\n",
    "#         image_data[dataset_name][difficulty][\"images gray\"] = images_gray\n",
    "#         image_data[dataset_name][difficulty][\"image names\"] = image_names\n",
    "    \n",
    "#     image_data[dataset_name][\"medium\"] = {}\n",
    "#     for datatype_name in image_data[dataset_name][\"easy\"]:\n",
    "#         image_data[dataset_name][\"medium\"][datatype_name] = image_data[dataset_name][\"easy\"][datatype_name] + image_data[dataset_name][\"hard\"][datatype_name]\n",
    "#         print(\"Combined {} {} for medium difficulty\".format(len(image_data[dataset_name][\"medium\"][datatype_name]), datatype_name))\n",
    "\n",
    "\n",
    "# image_data[\"roxford5k\"][\"easy\"][\"image names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_retrieval_k(train_data, test_data, train_names, test_names, train_images_as_array, test_images_as_array, k=20, view_option=0, image_size=(32,32), border_size=20):\n",
    "    avg_precisions = []\n",
    "    avg_recalls = []\n",
    "    precisionsatk = []\n",
    "    count = 0\n",
    "    \n",
    "    for idx, query in enumerate(test_data):\n",
    "        \n",
    "        all_precisions = []\n",
    "        all_recalls = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "\n",
    "        # Finding the euclidean distance from the query image and sorting them into index\n",
    "        query = query.reshape((1, -1))\n",
    "        D = euclidean_distances(train_data, query).squeeze()\n",
    "        index = np.argsort(D)\n",
    "        \n",
    "        # Finding the index of the last correct image in the sorted index to iter to\n",
    "        last_correct_image_idx = 0\n",
    "        for i in range(len(index)):\n",
    "            if train_names[index[i]] == test_names[idx]:\n",
    "                last_correct_image_idx = i\n",
    "        \n",
    "        # make sure we iter to k (for precision@k) if all correct images are found before k\n",
    "        if k > last_correct_image_idx:\n",
    "            last_correct_image_idx = k+1\n",
    "        \n",
    "        # Itering through all images untill we get to k or last correct image to compute AP\n",
    "        for kk in range(1, last_correct_image_idx+2):\n",
    "            TP = 0\n",
    "            FP = 0\n",
    "            FN = 0\n",
    "            \n",
    "            # Finding the correct amount of images in the training set\n",
    "            correct_count = 0\n",
    "            for ind in index:\n",
    "                if train_names[ind] == test_names[idx]:\n",
    "                    correct_count += 1\n",
    "            sized_index = index[:kk]\n",
    "            \n",
    "            # Find TP FP FN\n",
    "            for ind in sized_index:\n",
    "                if train_names[ind] == test_names[idx]:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    FP += 1\n",
    "            FN = correct_count - TP\n",
    "            \n",
    "            # If we want to view the images then we run this code, else its a waste of computational time\n",
    "            if view_option == 1:\n",
    "                # Creating image of k images (including query image at start)\n",
    "                tmp = [query.reshape(image_size)]\n",
    "                for ind in sized_index[:k]:\n",
    "                    tmp.append(train_data[ind].reshape(image_size))\n",
    "                output = np.array(tmp)*255\n",
    "                output = output.transpose(1, 0, 2)\n",
    "                output = output.reshape((image_size[0], -1))\n",
    "                im_query = Image.fromarray(output)\n",
    "            \n",
    "            # If the last k image is a correct image we add precision to the list\n",
    "            if train_names[sized_index[-1]] == test_names[idx]:\n",
    "                precisions.append(TP/(TP+FP))\n",
    "                recalls.append(TP/(TP+FN))\n",
    "\n",
    "            # Adding all precisions and recalls to a seperate list\n",
    "            all_precisions.append(TP/(TP+FP))\n",
    "            all_recalls.append(TP/(TP+FN))\n",
    "        \n",
    "     \n",
    "        # Solving AP, AR and precision@k\n",
    "        avg_precisions.append(np.average(precisions))\n",
    "        avg_recalls.append(np.average(all_recalls))\n",
    "        precisionsatk.append(all_precisions[k-1])\n",
    "        \n",
    "        # Set a viewing option, if 1 we print out the following:\n",
    "        if view_option == 1:\n",
    "            display(im_query) \n",
    "            print(\"Label: {}\".format(test_names[idx]))\n",
    "            print(\"Average Precision for query {}: \".format(idx), avg_precisions[-1])\n",
    "            print(\"Precision@k for query {}: \".format(idx), precisionsatk[-1])\n",
    "            print(\"\\n\")\n",
    "        elif view_option == 0:\n",
    "            count += 1 \n",
    "            print(\"Percentage Complete: {}\".format(round((count/len(test_data))*100),2), end=\"\\r\")\n",
    "        elif view_option == 2:\n",
    "            # Allowing a view_option 2 -> for viewing top k images from non_pixel value inputs\n",
    "            # creating an array of the top k similar images\n",
    "            top_k_images = [test_images_as_array[idx]]\n",
    "            for i in range(0,k):\n",
    "                top_k_images.append(train_images_as_array[index[i]])\n",
    "\n",
    "            fig, axes = plt.subplots(1, k+1, figsize=(200/k, 200/k))\n",
    "            for i, (image, ax) in enumerate(zip(top_k_images, axes.ravel())):\n",
    "                # convert image to RGB and add border:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                # resize image if border size greater than 10:\n",
    "                if border_size >= 10:\n",
    "                    image = cv2.resize(image, (250, 400), interpolation=cv2.INTER_CUBIC)\n",
    "                if i == 0:\n",
    "                    query_name = test_names[idx]\n",
    "                    title = \"Query: {}\".format(query_name)\n",
    "                    color = (0, 255, 0)\n",
    "                    image = border(image, color, border_size)\n",
    "                else:\n",
    "                    title = train_names[sized_index[i-1]]\n",
    "                    if train_names[sized_index[i-1]] == query_name:\n",
    "                        color = (0, 255, 0)\n",
    "                        image = border(image, color, border_size)\n",
    "                    else:\n",
    "                        color = (255, 0, 0)\n",
    "                        image = border(image, color, border_size)\n",
    "                # display all set options\n",
    "                ax.imshow(image, cmap=\"gray\")\n",
    "                ax.set_title(title)\n",
    "                ax.axis(\"off\")\n",
    "            plt.show()\n",
    "            print(\"Label: {}\".format(test_names[idx]))\n",
    "            print(\"Average Precision for query {}: \".format(idx), avg_precisions[-1])\n",
    "            print(\"Precision@k for query {}: \".format(idx), precisionsatk[-1])\n",
    "            print(\"\\n\")\n",
    "        elif view_option == 3:\n",
    "            top_k_images = [test_images_as_array[idx]]\n",
    "            for i in range(0,k):\n",
    "                top_k_images.append(train_images_as_array[index[i]])\n",
    "\n",
    "            fig, axes = plt.subplots(1, k+1, figsize=(200/k, 200/k))\n",
    "            for i, (image, ax) in enumerate(zip(top_k_images, axes.ravel())):\n",
    "                # convert image to RGB and add border:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                # resize image if border size greater than 10:\n",
    "                if border_size >= 10:\n",
    "                    image = cv2.resize(image, (250, 400), interpolation=cv2.INTER_CUBIC)\n",
    "                if i == 0:\n",
    "                    query_name = test_names[idx]\n",
    "                    title = \"Query: {}\".format(query_name)\n",
    "                else:\n",
    "                    title = train_names[sized_index[i-1]]\n",
    "                    if train_names[sized_index[i-1]] == query_name:\n",
    "                        color = (0, 255, 0)\n",
    "                        image = border(image, color, border_size)\n",
    "                    else:\n",
    "                        color = (255, 0, 0)\n",
    "                        image = border(image, color, border_size)\n",
    "                # display all set options\n",
    "                ax.imshow(image, cmap=\"gray\")\n",
    "                ax.set_title(title)\n",
    "                ax.axis(\"off\")\n",
    "            plt.show()\n",
    "    return avg_precisions, avg_recalls, precisionsatk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save metrics data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_csv(_precisionsatk, _AP, _k, _dataset_name):\n",
    "    data = {'Precision@k': _precisionsatk, 'Average Precision': _AP}\n",
    "    df = pandas.DataFrame(data=data)\n",
    "    pandas.set_option(\"display.max_rows\", 500, \"display.max_columns\", 4)\n",
    "    df.to_csv('{}-metrics_k={}.csv'.format(_dataset_name, _k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting a border around an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def border(img, color, border_size):\n",
    "    # get dimensions\n",
    "    h, w = img.shape[0:2]\n",
    "\n",
    "    # make a base slightly bigger than image\n",
    "    base_size= h+(border_size*2), w+(border_size*2), 3\n",
    "    base = np.zeros(base_size, dtype=np.uint8)\n",
    "\n",
    "    # make a boundary of chosen color\n",
    "    cv2.rectangle(base, (0,0), (w+20,h+20), color, 30)\n",
    "\n",
    "    # put original image into base\n",
    "    base[border_size:h+border_size, border_size:w+border_size] = img\n",
    "    plt.imshow(base)\n",
    "    \n",
    "    return base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving SIFT + BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIFT(images):\n",
    "    sift = cv2.SIFT_create()\n",
    "    \n",
    "    keypoints_per_image = []\n",
    "    descriptor_per_image = []\n",
    "    \n",
    "    count = 0\n",
    "    for image in images:\n",
    "        keypoints, descriptor = sift.detectAndCompute(image, None)\n",
    "\n",
    "        keypoints_per_image.append(keypoints)\n",
    "        descriptor_per_image.append(descriptor)\n",
    "        \n",
    "        count += 1 \n",
    "        print(\"Percentage Completed: {}%\".format(round((count/len(images))*100), 2), end=\"\\r\")\n",
    "    \n",
    "    print(\"\")\n",
    "    return keypoints_per_image, descriptor_per_image\n",
    "\n",
    "def stack_descriptors(descriptors):\n",
    "    stack = []\n",
    "    \n",
    "    for desc in descriptors:\n",
    "        tmp = np.array(desc)\n",
    "        if tmp.shape:\n",
    "            stack.append(tmp)\n",
    "            \n",
    "    all_descriptors = np.vstack(i for i in stack)\n",
    "    \n",
    "    return all_descriptors\n",
    "\n",
    "def cluster(data, n_clusters=100, cluster_type=\"minibatch\"):\n",
    "    start = time.time()\n",
    "    \n",
    "    if cluster_type == \"minibatch\":\n",
    "        cluster = MiniBatchKMeans(n_clusters=n_clusters)\n",
    "        y_cluster = cluster.fit(data)\n",
    "    elif cluster_type == \"kmeans\":\n",
    "        cluster = KMeans(n_clusters=n_clusters)\n",
    "        y_cluster = cluster.fit(data)\n",
    "    else:\n",
    "        print(\"Unknown cluster_type! Try: 'minibatch' or 'kmeans'\")\n",
    "        \n",
    "    end = time.time()\n",
    "    print(\"Time Elapsed: {} min\".format(round((end - start)/60, 2)))\n",
    "    return y_cluster\n",
    "\n",
    "def solve_BoW(descriptors, y_cluster, n_clusters):\n",
    "    previous = 0\n",
    "    count = 0\n",
    "    image_words = []\n",
    "    for image_number in range(len(descriptors)):\n",
    "        if descriptors[image_number] is not None:\n",
    "            tmp = []\n",
    "            for kp in descriptors[image_number]:\n",
    "                cluster = y_cluster.predict(np.array([kp]))\n",
    "                tmp.append(cluster[0])\n",
    "            image_words.append(tmp)\n",
    "            \n",
    "            count += 1\n",
    "            print(\"(1/2) Percentage Completed: {}%\".format(round((count/len(descriptors))*100), 2), end=\"\\r\")\n",
    "        else:\n",
    "            # If image has no desciptors, append 0 words to it\n",
    "            image_words.append([0])\n",
    "    \n",
    "    print(\"\")\n",
    "    count = 0\n",
    "    image_histograms = []\n",
    "    for image in range(len(image_words)):\n",
    "        hist = np.zeros(n_clusters)\n",
    "        for words in image_words[image]:\n",
    "            hist[words-1] = hist[words-1]+1\n",
    "        image_histograms.append(hist)\n",
    "        \n",
    "        count += 1\n",
    "        print(\"(2/2) Percentage Completed: {}%\".format(round((count/len(image_words))*100), 2), end=\"\\r\")\n",
    "    \n",
    "    print(\"\")\n",
    "    # Transforming data using tf-idf:\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    weighted_image_histograms = transformer.fit_transform(image_histograms).toarray()\n",
    "    \n",
    "    return weighted_image_histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oxford 5k SIFT Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing test SIFT features...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'image_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5518/2373584083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Copmuting bovw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nComputing test SIFT features...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_kp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSIFT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"roxford5k\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images gray\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nComputing train SIFT features...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_kp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSIFT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"roxford5k\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"easy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images gray\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_data' is not defined"
     ]
    }
   ],
   "source": [
    "n_clusters = 100\n",
    "\n",
    "# Copmuting bovw\n",
    "print(\"\\nComputing test SIFT features...\")\n",
    "test_kp, test_desc = SIFT(image_data[\"roxford5k\"][\"query\"][\"images gray\"])\n",
    "print(\"\\nComputing train SIFT features...\")\n",
    "train_kp, train_desc = SIFT(image_data[\"roxford5k\"][\"easy\"][\"images gray\"])\n",
    "stacked_train_desc = stack_descriptors(train_desc)\n",
    "\n",
    "print(\"\\nClustering Descriptors...\")\n",
    "cluster_func = cluster(stacked_train_desc, n_clusters)\n",
    "\n",
    "print(\"\\nComputing test BoVW...\")\n",
    "test_bovw  = solve_BoW(test_desc, cluster_func, n_clusters)\n",
    "print(\"\\nComputing train BoVW...\")\n",
    "train_bovw = solve_BoW(train_desc, cluster_func, n_clusters)\n",
    "\n",
    "# Compute metrics\n",
    "\n",
    "train_names = image_data[\"roxford5k\"][\"easy\"][\"image names\"]\n",
    "test_names = image_data[\"roxford5k\"][\"query\"][\"image names\"]\n",
    "train_colour_images = image_data[\"roxford5k\"][\"easy\"][\"images colour\"]\n",
    "test_colour_images = image_data[\"roxford5k\"][\"query\"][\"images colour\"]\n",
    "\n",
    "\n",
    "print(\"\\nComputing Metrics...\")\n",
    "AP, AR, precisionsatk = image_retrieval_k(train_bovw, test_bovw, train_names, test_names, train_colour_images, test_colour_images, k = 10, view_option = 0, border_size=20)\n",
    "\n",
    "# Display mAP\n",
    "mAP = np.average(AP)\n",
    "print(\"\\nmAP =\", mAP)\n",
    "\n",
    "# Save data\n",
    "# if savedata == 1:\n",
    "#     save_data_to_csv(precisionsatk, AP, k, \"Oxford5k_BoVW_{}\".format(n_clusters))\n",
    "#     print(\"\\nData saved to csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIFT_and_save(train_images_dir, test_images_dir, n_clusters, test_save_stem, train_save_stem, dir = \"/home/sean/Code/Pawsey/3. Data/New_SIFT\"):\n",
    "    # Loading images\n",
    "    train_images, train_names = load_images(train_images_dir)\n",
    "    test_images, test_names = load_images(test_images_dir)\n",
    "\n",
    "    # Copmuting bovw\n",
    "    print(\"\\nComputing test SIFT features...\")\n",
    "    test_kp, test_desc = SIFT(test_images)\n",
    "    print(\"\\nComputing train SIFT features...\")\n",
    "    train_kp, train_desc = SIFT(train_images)\n",
    "    stacked_train_desc = stack_descriptors(train_desc)\n",
    "\n",
    "    print(\"\\nClustering Descriptors...\")\n",
    "    cluster_func = cluster(stacked_train_desc, n_clusters)\n",
    "\n",
    "    print(\"\\nComputing test BoVW...\")\n",
    "    test_bovw  = solve_BoW(test_desc, cluster_func, n_clusters)\n",
    "    print(\"\\nComputing train BoVW...\")\n",
    "    train_bovw = solve_BoW(train_desc, cluster_func, n_clusters)\n",
    "\n",
    "    print(\"\\nSaving data\")\n",
    "    os.chdir(dir)\n",
    "    test_bovw = pd.DataFrame(test_bovw)\n",
    "    test_bovw.to_csv(\"SIFT/\" + test_save_stem + \"SIFT.csv\", index=False)\n",
    "    pd.DataFrame(train_bovw).to_csv(\"SIFT/\" + train_save_stem + \"SIFT.csv\", index=False)\n",
    "    \n",
    "    np.save(\"names/\" + test_save_stem + \"names.npy\", test_names)\n",
    "    np.save(\"names/\" + train_save_stem + \"names.npy\", train_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 516 images from /home/sean/Code/Pawsey/3. Data/Revised and Sorted/roxford5k/easy\n",
      "Loaded 70 images from /home/sean/Code/Pawsey/3. Data/Revised and Sorted/roxford5k/query\n",
      "\n",
      "Computing test SIFT features...\n",
      "Percentage Completed: 100%\n",
      "\n",
      "Computing train SIFT features...\n",
      "Percentage Completed: 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5518/2309145608.py:28: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  all_descriptors = np.vstack(i for i in stack)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering Descriptors...\n",
      "Time Elapsed: 4.06 min\n",
      "\n",
      "Computing test BoVW...\n",
      "(1/2) Percentage Completed: 100%\n",
      "(2/2) Percentage Completed: 100%\n",
      "\n",
      "Computing train BoVW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/2) Percentage Completed: 100%\n",
      "(2/2) Percentage Completed: 100%\n",
      "\n",
      "Saving data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 685 images from /home/sean/Code/Pawsey/3. Data/Revised and Sorted/roxford5k/hard\n",
      "Loaded 70 images from /home/sean/Code/Pawsey/3. Data/Revised and Sorted/roxford5k/query\n",
      "\n",
      "Computing test SIFT features...\n",
      "Percentage Completed: 100%\n",
      "\n",
      "Computing train SIFT features...\n",
      "Percentage Completed: 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5518/2309145608.py:28: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  all_descriptors = np.vstack(i for i in stack)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering Descriptors...\n",
      "Time Elapsed: 4.18 min\n",
      "\n",
      "Computing test BoVW...\n",
      "(1/2) Percentage Completed: 100%\n",
      "(2/2) Percentage Completed: 100%\n",
      "\n",
      "Computing train BoVW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/2) Percentage Completed: 100%\n",
      "(2/2) Percentage Completed: 100%\n",
      "\n",
      "Saving data\n",
      "Loaded 1470 images from /home/sean/Code/Pawsey/3. Data/Revised and Sorted/rparis6k/easy\n",
      "Loaded 70 images from /home/sean/Code/Pawsey/3. Data/Revised and Sorted/rparis6k/query\n",
      "\n",
      "Computing test SIFT features...\n",
      "Percentage Completed: 100%\n",
      "\n",
      "Computing train SIFT features...\n",
      "Percentage Completed: 4%\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5518/4156216089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtrain_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasedir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdifficulty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtest_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasedir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/query\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         SIFT_and_save(train_images_dir = train_dir,\n\u001b[0m\u001b[1;32m      8\u001b[0m                       \u001b[0mtest_images_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                       \u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5518/1239798024.py\u001b[0m in \u001b[0;36mSIFT_and_save\u001b[0;34m(train_images_dir, test_images_dir, n_clusters, test_save_stem, train_save_stem, dir)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtest_kp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSIFT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nComputing train SIFT features...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_kp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSIFT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mstacked_train_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack_descriptors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5518/2309145608.py\u001b[0m in \u001b[0;36mSIFT\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mkeypoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mkeypoints_per_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "basedir = \"/home/sean/Code/Pawsey/3. Data/Revised and Sorted/\"\n",
    "n_clusters = 10000\n",
    "for dataset_name in [\"roxford5k\", \"rparis6k\"]:\n",
    "    for difficulty in [\"easy\", \"hard\"]:\n",
    "        train_dir = basedir + dataset_name + \"/\" + difficulty\n",
    "        test_dir = basedir + dataset_name + \"/query\"\n",
    "        SIFT_and_save(train_images_dir = train_dir,\n",
    "                      test_images_dir = test_dir,\n",
    "                      n_clusters = n_clusters,\n",
    "                      test_save_stem= \"{}-{}-query{}-\".format(dataset_name, n_clusters, difficulty[0]),\n",
    "                      train_save_stem = \"{}-{}-{}-\".format(dataset_name, n_clusters, difficulty))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images_folder -> path to directory with all MPEG7 images\n",
    "\n",
    "images_path -> path to directory with all oxford building images\n",
    "\n",
    "gt_path -> path to directory with all ground truth files\n",
    "\n",
    "pixelorsift -> Choose either \"pixel\" or \"sift\", runs that code\n",
    "\n",
    "savedata -> Saves metrics to csv\n",
    "\n",
    "n_clusters -> number of words for SIFT\n",
    "\n",
    "k -> number of returned images (also k images checked in precision at k)\n",
    "\n",
    "view_option:\n",
    " - 0 -> returns only mAP\n",
    " - 1 -> returns merged images and metrics (only for database of same sized images) (doesnt work with SIFT)\n",
    " - 2 -> returns images and metrics (coloured, any size, labelled)\n",
    " - 3 -> returns images only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
