{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Results of Analysis\n",
    "\n",
    "Combine SIFT, resnet, swin, vgg and vit features with basic distance analysis, query expansion and diffusion.\n",
    "\n",
    "____\n",
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn.metrics.pairwise\n",
    "import my_eval\n",
    "import query\n",
    "\n",
    "###########################################\n",
    "\n",
    "NOTEBOOK_DIR = \"/home/sean/Code/Pawsey/4. Clean\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded swin\n",
      "Loaded vgg\n",
      "Loaded resnet\n",
      "Loaded names\n",
      "Loaded vit\n",
      "Loaded names\n",
      "Loaded sift-10k\n",
      "Loaded sift-1k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.4381919 , -1.1369115 , -0.49100572, ..., -0.27456677,\n",
       "         0.38102797, -0.30554023],\n",
       "       [-0.19804995,  0.02098738,  0.52111053, ...,  0.44540596,\n",
       "         0.8620084 ,  0.18907186],\n",
       "       [ 1.0216093 , -0.06300209, -0.06569103, ...,  0.02202551,\n",
       "        -0.32440802,  0.3858102 ],\n",
       "       ...,\n",
       "       [ 0.74518114, -0.9655011 , -0.55623275, ..., -0.39560622,\n",
       "         0.3983633 , -0.4672271 ],\n",
       "       [ 0.4493655 , -0.97439206, -0.61376625, ..., -0.19914342,\n",
       "         0.27447924, -0.3482531 ],\n",
       "       [ 0.04259995,  0.09633142,  0.65417933, ...,  0.5438953 ,\n",
       "         0.53027916,  0.03832415]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load features\n",
    "\n",
    "data = {}\n",
    "\n",
    "for data_subset in [\"oldenburger\", \"sutton\"]:\n",
    "    subdir = \"./data/\" + data_subset\n",
    "    data[data_subset] = {}\n",
    "\n",
    "    for descriptor in os.listdir(subdir):\n",
    "\n",
    "        if descriptor == \"names\":\n",
    "            data[data_subset][descriptor] = {\"ox\" : {}, \"par\" : {}}\n",
    "            for fname in os.listdir(subdir + \"/\" + descriptor):\n",
    "                split_name = fname[:-4].split(\"-\")\n",
    "                dataset = split_name[0]\n",
    "                if fname.endswith(\"y-names.npy\"):\n",
    "                    data[data_subset][descriptor][dataset][\"y\"] = np.load(\"./data/{}/{}/{}\".format(data_subset, descriptor, fname))\n",
    "                else:\n",
    "                    difficulty = split_name[2]\n",
    "                    data[data_subset][descriptor][dataset][difficulty] = np.load(\"./data/{}/{}/{}\".format(data_subset, descriptor, fname))\n",
    "\n",
    "        else:\n",
    "            data[data_subset][descriptor] = {\"ox\" : {\"E\" : {}, \"M\" : {}, \"H\" :{}},\n",
    "                                \"par\" : {\"E\" : {}, \"M\" : {}, \"H\" :{}}}\n",
    "            for fname in os.listdir(subdir + \"/\" + descriptor):\n",
    "                split_name = fname[:-4].split(\"-\")\n",
    "                if len(split_name) == 3:\n",
    "                    pass\n",
    "                else:\n",
    "                    (_, xy, dataset, difficulty) = split_name\n",
    "                    data[data_subset][descriptor][dataset][difficulty][xy] = np.load(\"./data/{}/{}/{}\".format(data_subset, descriptor, fname))\n",
    "        \n",
    "        print(\"Loaded \" + descriptor)\n",
    "\n",
    "data[\"oldenburger\"][\"swin\"][\"ox\"][\"E\"][\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "## Evaluate Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_types': [],\n",
       " 'datasets': [],\n",
       " 'difficulties': [],\n",
       " 'query_types': [],\n",
       " 'm_APs': [],\n",
       " 'alpha': [],\n",
       " 'diffusion_scalar': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = {key : [] for key in [\"feature_type\", \"dataset\", \"difficulty\", \"query type\", \"mAP\", \"alpha\", \"diffusion scalar\"]}\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5, 5, 5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis = [1,2,3]\n",
    "lis += [5] * 3\n",
    "lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ox E swin\n",
      "[cache] loading ./tmp/ox_swin_E/offline.jbl costs 0.00s\n",
      "Processing ox M swin\n",
      "[cache] loading ./tmp/ox_swin_M/offline.jbl costs 0.00s\n",
      "Processing ox H swin\n",
      "[cache] loading ./tmp/ox_swin_H/offline.jbl costs 0.00s\n",
      "Processing par E swin\n",
      "[cache] loading ./tmp/par_swin_E/offline.jbl costs 0.00s\n",
      "Processing par M swin\n",
      "[cache] loading ./tmp/par_swin_M/offline.jbl costs 0.01s\n",
      "Processing par H swin\n",
      "[cache] loading ./tmp/par_swin_H/offline.jbl costs 0.00s\n",
      "Processing ox E vgg\n",
      "[cache] loading ./tmp/ox_vgg_E/offline.jbl costs 0.00s\n",
      "Processing ox M vgg\n",
      "[cache] loading ./tmp/ox_vgg_M/offline.jbl costs 0.00s\n",
      "Processing ox H vgg\n",
      "[cache] loading ./tmp/ox_vgg_H/offline.jbl costs 0.00s\n",
      "Processing par E vgg\n",
      "[cache] loading ./tmp/par_vgg_E/offline.jbl costs 0.00s\n",
      "Processing par M vgg\n",
      "[cache] loading ./tmp/par_vgg_M/offline.jbl costs 0.00s\n",
      "Processing par H vgg\n",
      "[cache] loading ./tmp/par_vgg_H/offline.jbl costs 0.00s\n",
      "Processing ox E resnet\n",
      "[cache] loading ./tmp/ox_resnet_E/offline.jbl costs 0.00s\n",
      "Processing ox M resnet\n",
      "[cache] loading ./tmp/ox_resnet_M/offline.jbl costs 0.00s\n",
      "Processing ox H resnet\n",
      "[cache] loading ./tmp/ox_resnet_H/offline.jbl costs 0.00s\n",
      "Processing par E resnet\n",
      "[cache] loading ./tmp/par_resnet_E/offline.jbl costs 0.00s\n",
      "Processing par M resnet\n",
      "[cache] loading ./tmp/par_resnet_M/offline.jbl costs 0.00s\n",
      "Processing par H resnet\n",
      "[cache] loading ./tmp/par_resnet_H/offline.jbl costs 0.00s\n",
      "Processing ox E vit\n",
      "[cache] loading ./tmp/ox_vit_E/offline.jbl costs 0.00s\n",
      "Processing ox M vit\n",
      "[cache] loading ./tmp/ox_vit_M/offline.jbl costs 0.00s\n",
      "Processing ox H vit\n",
      "[cache] loading ./tmp/ox_vit_H/offline.jbl costs 0.00s\n",
      "Processing par E vit\n",
      "[cache] loading ./tmp/par_vit_E/offline.jbl costs 0.00s\n",
      "Processing par M vit\n",
      "[cache] loading ./tmp/par_vit_M/offline.jbl costs 0.00s\n",
      "Processing par H vit\n",
      "[cache] loading ./tmp/par_vit_H/offline.jbl costs 0.00s\n",
      "Processing ox E sift-10k\n",
      "[cache] loading ./tmp/ox_sift-10k_E/offline.jbl costs 0.00s\n",
      "Processing ox M sift-10k\n",
      "[cache] loading ./tmp/ox_sift-10k_M/offline.jbl costs 0.00s\n",
      "CRASH! Running diffusion x10.\n",
      "[cache] loading ./tmp/ox_sift-10k_M_x10/offline.jbl costs 0.00s\n",
      "Processing ox H sift-10k\n",
      "[cache] loading ./tmp/ox_sift-10k_H/offline.jbl costs 0.00s\n",
      "Processing par E sift-10k\n",
      "[cache] loading ./tmp/par_sift-10k_E/offline.jbl costs 0.00s\n",
      "Processing par M sift-10k\n",
      "[cache] loading ./tmp/par_sift-10k_M/offline.jbl costs 0.00s\n",
      "CRASH! Running diffusion x10.\n",
      "[cache] loading ./tmp/par_sift-10k_M_x10/offline.jbl costs 0.00s\n",
      "CRASH! Running diffusion x50.\n",
      "[cache] loading ./tmp/par_sift-10k_M_x50/offline.jbl costs 0.01s\n",
      "Processing par H sift-10k\n",
      "[cache] loading ./tmp/par_sift-10k_H/offline.jbl costs 0.00s\n",
      "CRASH! Running diffusion x10.\n",
      "[cache] loading ./tmp/par_sift-10k_H_x10/offline.jbl costs 0.00s\n",
      "Processing ox E sift-1k\n",
      "[cache] loading ./tmp/ox_sift-1k_E/offline.jbl costs 0.00s\n",
      "Processing ox M sift-1k\n",
      "[cache] loading ./tmp/ox_sift-1k_M/offline.jbl costs 0.00s\n",
      "Processing ox H sift-1k\n",
      "[cache] loading ./tmp/ox_sift-1k_H/offline.jbl costs 0.00s\n",
      "Processing par E sift-1k\n",
      "[cache] loading ./tmp/par_sift-1k_E/offline.jbl costs 0.00s\n",
      "Processing par M sift-1k\n",
      "[cache] loading ./tmp/par_sift-1k_M/offline.jbl costs 0.00s\n",
      "Processing par H sift-1k\n",
      "[cache] loading ./tmp/par_sift-1k_H/offline.jbl costs 0.00s\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "kappas = [1,5,10]\n",
    "alphas = [0.25, 0.5, 0.75, 1]\n",
    "distance_metrics = {\"euclidean\" : sklearn.metrics.pairwise.euclidean_distances, \"cosine\" : sklearn.metrics.pairwise.cosine_distances}\n",
    "\n",
    "# evaluate data\n",
    "r = {key : [] for key in [\"feature_type\", \"dataset\", \"difficulty\", \"query type\", \"alpha\", \"diffusion scalar\", \"distance metric\", \"mAP\"]}\n",
    "for kappa in kappas:\n",
    "    r[\"precision at \" + str(kappa)] = []\n",
    "\n",
    "for data_split in data:\n",
    "    for feature in data[data_split]:\n",
    "        \n",
    "        if feature == \"names\":\n",
    "            continue\n",
    "        \n",
    "        for dataset in [\"ox\", \"par\"]:\n",
    "            for (difficulty, dat) in data[data_split][feature][dataset].items():\n",
    "\n",
    "                print(\"Processing {} {} {}\".format(dataset, difficulty, feature))\n",
    "\n",
    "                queries = dat[\"y\"]\n",
    "                gallery = dat[\"x\"]\n",
    "                query_names = data[data_split][\"names\"][dataset][\"y\"]\n",
    "                gallery_names = data[data_split][\"names\"][dataset][difficulty]\n",
    "\n",
    "                # Compute basic query and expanded query\n",
    "\n",
    "                for (metric_name, metric_function) in distance_metrics.items():\n",
    "                    n_tests = len(alphas) + 1\n",
    "                    r[\"feature_type\"] += [feature] * n_tests\n",
    "                    r[\"dataset\"] += [dataset] * n_tests\n",
    "                    r[\"difficulty\"] += [difficulty] * n_tests\n",
    "                    r[\"query type\"] += [\"basic\"] + [\"expanded\"] * len(alphas)\n",
    "                    r[\"alpha\"] += [np.nan] + alphas\n",
    "                    r[\"distance metric\"] += [metric_name] * n_tests\n",
    "                    r[\"diffusion scalar\"] += [np.nan] * n_tests\n",
    "\n",
    "                    basic_ranks = query.return_ranks('basic', queries, gallery, metric_function = metric_function)\n",
    "                    m_ap, ps = my_eval.evaluate(basic_ranks, query_names, gallery_names, kappas)\n",
    "                    r[\"mAP\"].append(m_ap)\n",
    "                    for (kappa, p) in ps.items():\n",
    "                        r[\"precision at \" + str(kappa)].append(p)\n",
    "                    \n",
    "                    for alpha in alphas:\n",
    "                        expansion_ranks = query.return_ranks('expanded', queries, gallery, alpha=alpha, metric_function = metric_function)\n",
    "                        m_ap, ps = my_eval.evaluate(expansion_ranks, query_names, gallery_names, kappas)\n",
    "                        r[\"mAP\"].append(m_ap)\n",
    "                        for (kappa, p) in ps.items():\n",
    "                            r[\"precision at \" + str(kappa)].append(p)\n",
    "                \n",
    "                # Compute diffusion query\n",
    "\n",
    "                r[\"feature_type\"].append(feature)\n",
    "                r[\"dataset\"].append(dataset)\n",
    "                r[\"difficulty\"].append(difficulty)\n",
    "                r[\"query type\"].append(\"diffusion\")\n",
    "                r[\"alpha\"].append(np.nan)\n",
    "                r[\"distance metric\"].append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    diffusion_ranks = query.return_ranks('diffusion', queries, gallery, cache_dir = \"./tmp/{}_{}_{}\".format(dataset, feature, difficulty))\n",
    "                    r[\"diffusion scalar\"].append(1)\n",
    "                except ValueError: # caused by the values being too small\n",
    "                    try:\n",
    "                        print(\"CRASH! Running diffusion x10.\")\n",
    "                        diffusion_ranks = query.return_ranks('diffusion', queries * 10, gallery * 10, cache_dir = \"./tmp/{}_{}_{}_x10\".format(dataset, feature, difficulty))\n",
    "                        r[\"diffusion scalar\"].append(10)\n",
    "                    except ValueError: # caused by the values being too small\n",
    "                        print(\"CRASH! Running diffusion x50.\")\n",
    "                        diffusion_ranks = query.return_ranks('diffusion', queries * 50, gallery * 50, cache_dir = \"./tmp/{}_{}_{}_x50\".format(dataset, feature, difficulty))\n",
    "                        r[\"diffusion scalar\"].append(10)\n",
    "\n",
    "                m_ap, ps = my_eval.evaluate(diffusion_ranks, query_names, gallery_names, kappas)\n",
    "                r[\"mAP\"].append(m_ap)\n",
    "                for (kappa, p) in ps.items():\n",
    "                    r[\"precision at \" + str(kappa)].append(p)\n",
    "\n",
    "results=pd.DataFrame(r)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results.to_csv(\"./results/first_full_set.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e0edef247045f2f5f35ac9d6435770b0c68a1ddd7eb34b4959830e587ac51e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
