{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sift Evaluator\n",
    "\n",
    "The objectives of this notebook are to:\n",
    "\n",
    "- Define functions that query the data based on different parameters (distance metric, transformations?)\n",
    "- Define functions to evaluate the truth of each returned query parameter\n",
    "- Define functions to calculate mAP and precision@k for the above output\n",
    "- Create a pipeline for evaluating the effects of different parameter set ups / transformations on mAP and p@k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "# Evaluation Functions\n",
    "____\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import unittest\n",
    "import sklearn.metrics.pairwise\n",
    "import sklearn.preprocessing\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "### Query Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_query(query_image_features, query_target_features, metric_function = sklearn.metrics.pairwise.euclidean_distances):\n",
    "    \"\"\"Return the indexes of the query_target images, arranged in ascending euclidean distance as compared to the query image\"\"\"\n",
    "    \n",
    "    query = query_image_features.reshape((1, -1))\n",
    "    D = metric_function(query_target_features, query).squeeze()\n",
    "    index = np.argsort(D)\n",
    "\n",
    "    return(index)\n",
    "\n",
    "def reranked_query():\n",
    "    \"\"\"reranking query goes here\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "### Query to Truth Value Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_results_to_truth_values(query_image_building, query_results, image_names):\n",
    "    \"\"\"Convert the index results of a query to an array of booleans corresponding to whether the correct image was retrieved.\"\"\"\n",
    "    return([query_image_building == image_names[index] for index in query_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "### Truth Value Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Potential Improvements\n",
    "# 1. Precision_at_k to print precision at last true value?\n",
    "# 2. Create a more computationally efficient (/combined?) version.\n",
    "# 3. Add recall_at_k\n",
    "\n",
    "def precision_at_k(truth_values, k, warnings=True):\n",
    "    \"\"\"Return proportions of true values in the first k elements.\n",
    "    If warnings=True and all true values occur before the kth element, raise an error\"\"\"\n",
    "    p_at_k = truth_values[:k].count(True) / k\n",
    "\n",
    "    if warnings:\n",
    "        if k < len(truth_values):\n",
    "            if truth_values[k:].count(True) == 0:\n",
    "                raise ValueError(\"All true values are before the first k values\")\n",
    "    \n",
    "    return(p_at_k)\n",
    "\n",
    "\n",
    "def average_precision(truth_values):\n",
    "    \"\"\"Given a boolean input of whether returned query values are correct or false, return the average precision.\n",
    "    e.g. average_precision([True, True, False, True]) ~ 0.85\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    for (index, val) in enumerate(truth_values):\n",
    "        if val: # == True\n",
    "            precisions.append(truth_values[:index + 1].count(True) / (index + 1))      \n",
    "\n",
    "    return(np.mean(precisions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "### Total Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_at_k = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function append:\n",
      "\n",
      "append(object, /) method of builtins.list instance\n",
      "    Append object to the end of the list.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help([].append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "a = precisions_at_k.get(1, 2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(train_features, test_features, train_names, test_names, query_function, metric_function, average_mean_precision = True, k_values = [5,10,20]):\n",
    "    \"\"\"Run each test feature against the train features,\"\"\"\n",
    "    average_precisions = []\n",
    "    precisions_at_k = {}\n",
    "    for k in k_values:\n",
    "        precisions_at_k[k] = []\n",
    "    \n",
    "    for (test_feature, test_feature_name) in zip(test_features.iterrows(), test_names):\n",
    "        query_image_features = test_feature[1].values # extract the values for the iterrows row object\n",
    "        query_results = query_function(query_image_features = query_image_features, query_target_features = train_features, metric_function = metric_function)\n",
    "        truth_values = query_results_to_truth_values(test_feature_name, query_results, train_names)\n",
    "        \n",
    "        average_precisions.append(average_precision(truth_values))\n",
    "        for k in precisions_at_k:\n",
    "            p_at_k = precision_at_k(truth_values, k, warnings=False)\n",
    "            precisions_at_k[k].append(p_at_k)\n",
    "    \n",
    "    for (k_value, list_of_precisions) in precisions_at_k.items():\n",
    "        precisions_at_k[k_value] = np.mean(list_of_precisions)\n",
    "    \n",
    "    return(np.mean(average_precisions), precisions_at_k)\n",
    "\n",
    "\n",
    "## Outdated version\n",
    "\n",
    "# def mean_average_precision(train_features, test_features, train_names, test_names, distance_metric):\n",
    "#     \"\"\"descriptive docstring do map yeah\"\"\"\n",
    "#     average_precisions = []\n",
    "    \n",
    "#     for (test_feature, test_feature_name) in zip(test_features.iterrows(), test_names):\n",
    "#         features_as_array = test_feature[1].values # extract the values for the iterrows row object\n",
    "#         query_results = basic_query(features_as_array, train_features, distance_metric)\n",
    "#         truth_values = query_results_to_truth_values(test_feature_name, query_results, train_names)\n",
    "#         average_precisions.append(average_precision(truth_values))\n",
    "    \n",
    "#     return(np.mean(average_precisions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n",
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_averageprecision (__main__.TestMetrics) ... ok\n",
      "test_precision_at_k (__main__.TestMetrics) ... ok\n",
      "test_basicquery (__main__.TestQuery) ... ok\n",
      "test_queryresultstotruthvalues (__main__.TestTruthValues) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fe3fd90a580>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestQuery(unittest.TestCase):\n",
    "\n",
    "    def test_basicquery(self):\n",
    "        test_query = np.array([0,0,0,0])\n",
    "        test_query_target = pd.DataFrame(data={\"a\" : [4,0,0,0],\n",
    "                             \"b\" : [0,1,2,0],\n",
    "                             \"c\" : [0,0,0,0],\n",
    "                             \"d\" : [0,0,0,6]})\n",
    "        self.assertTrue(np.allclose(basic_query(test_query, test_query_target), [1,2,0,3]))\n",
    "\n",
    "class TestTruthValues(unittest.TestCase):\n",
    "\n",
    "    def test_queryresultstotruthvalues(self):\n",
    "        test_query_image_building = \"A\"\n",
    "        test_query_results = [0,4,3,2,1,5,6]\n",
    "        test_image_names = [\"A\", \"B\", \"C\", \"D\", \"A\", \"A\", \"D\"]\n",
    "        results = query_results_to_truth_values(test_query_image_building, test_query_results, test_image_names)\n",
    "        self.assertTrue(np.array_equal(results, [True, True, False, False, False, True, False]))\n",
    "\n",
    "class TestMetrics(unittest.TestCase):\n",
    "\n",
    "    def test_precision_at_k(self):\n",
    "        self.assertEqual(precision_at_k([True,False,True, True],4), 0.75)\n",
    "        self.assertEqual(precision_at_k([True, False, True, False, True], 2), 0.5)\n",
    "\n",
    "    def test_averageprecision(self):\n",
    "        self.assertAlmostEqual(average_precision([True, False, True, True]), np.mean([1,2/3,3/4]))\n",
    "        self.assertAlmostEqual(average_precision([False, False, True, False, True]), np.mean([1/3,2/5]))\n",
    "    \n",
    "    # def test_meanaverageprecision(self)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.379250</td>\n",
       "      <td>0.124875</td>\n",
       "      <td>0.319125</td>\n",
       "      <td>0.205812</td>\n",
       "      <td>0.183844</td>\n",
       "      <td>0.361906</td>\n",
       "      <td>0.250906</td>\n",
       "      <td>0.449781</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>0.394281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.323421</td>\n",
       "      <td>0.171378</td>\n",
       "      <td>0.285630</td>\n",
       "      <td>0.188076</td>\n",
       "      <td>0.267174</td>\n",
       "      <td>0.565107</td>\n",
       "      <td>0.249596</td>\n",
       "      <td>0.313753</td>\n",
       "      <td>0.304965</td>\n",
       "      <td>0.323421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.298072</td>\n",
       "      <td>0.195457</td>\n",
       "      <td>0.276083</td>\n",
       "      <td>0.195457</td>\n",
       "      <td>0.305401</td>\n",
       "      <td>0.559495</td>\n",
       "      <td>0.309066</td>\n",
       "      <td>0.285856</td>\n",
       "      <td>0.273640</td>\n",
       "      <td>0.316396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.406106</td>\n",
       "      <td>0.192947</td>\n",
       "      <td>0.273800</td>\n",
       "      <td>0.302589</td>\n",
       "      <td>0.258487</td>\n",
       "      <td>0.199684</td>\n",
       "      <td>0.336891</td>\n",
       "      <td>0.485123</td>\n",
       "      <td>0.294014</td>\n",
       "      <td>0.298914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.382649</td>\n",
       "      <td>0.202579</td>\n",
       "      <td>0.280109</td>\n",
       "      <td>0.296366</td>\n",
       "      <td>0.217585</td>\n",
       "      <td>0.437671</td>\n",
       "      <td>0.257600</td>\n",
       "      <td>0.335131</td>\n",
       "      <td>0.290113</td>\n",
       "      <td>0.380148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.379250  0.124875  0.319125  0.205812  0.183844  0.361906  0.250906   \n",
       "1  0.323421  0.171378  0.285630  0.188076  0.267174  0.565107  0.249596   \n",
       "2  0.298072  0.195457  0.276083  0.195457  0.305401  0.559495  0.309066   \n",
       "3  0.406106  0.192947  0.273800  0.302589  0.258487  0.199684  0.336891   \n",
       "4  0.382649  0.202579  0.280109  0.296366  0.217585  0.437671  0.257600   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.449781  0.333000  0.394281  \n",
       "1  0.313753  0.304965  0.323421  \n",
       "2  0.285856  0.273640  0.316396  \n",
       "3  0.485123  0.294014  0.298914  \n",
       "4  0.335131  0.290113  0.380148  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = \"/home/sean/Code/Pawsey/oxford_data/\"\n",
    "\n",
    "def load_oxford_5k_data(name):\n",
    "    \"\"\"Returns a dictionary of attributes and 1 of features for train or test data\"\"\"\n",
    "\n",
    "    attributes = {}\n",
    "    \n",
    "    for img_att in [\"names\", \"pixels\", \"images\"]:\n",
    "        attributes[img_att] = np.load(BASE_DIR + name + \"_\" + img_att + \".npy\", allow_pickle=True)\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    os.chdir(BASE_DIR + \"NPY files for BoVW/\")\n",
    "    for bovw_size in os.listdir():\n",
    "        features[bovw_size] = pd.DataFrame(np.load(bovw_size + \"/BoW_\" + name.capitalize() + \".npy\"))\n",
    "    \n",
    "    return(attributes, features)\n",
    "\n",
    "oxford5k = {\"raw features\" : {}, \"attributes\" : {}}\n",
    "(oxford5k[\"attributes\"][\"test\"], oxford5k[\"raw features\"][\"test\"]) = load_oxford_5k_data(\"test\")\n",
    "(oxford5k[\"attributes\"][\"train\"], oxford5k[\"raw features\"][\"train\"]) = load_oxford_5k_data(\"train\")\n",
    "\n",
    "oxford5k[\"raw features\"][\"test\"][\"bovw files for 10 Words\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "### Apply Vertical and Horizontal Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033827</td>\n",
       "      <td>0.035713</td>\n",
       "      <td>0.030498</td>\n",
       "      <td>0.018938</td>\n",
       "      <td>0.022437</td>\n",
       "      <td>0.022456</td>\n",
       "      <td>0.077555</td>\n",
       "      <td>0.051392</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>0.056622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046860</td>\n",
       "      <td>0.058236</td>\n",
       "      <td>0.041051</td>\n",
       "      <td>0.047852</td>\n",
       "      <td>0.024786</td>\n",
       "      <td>0.018312</td>\n",
       "      <td>0.036336</td>\n",
       "      <td>0.060646</td>\n",
       "      <td>0.023192</td>\n",
       "      <td>0.043599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011270</td>\n",
       "      <td>0.065442</td>\n",
       "      <td>0.036901</td>\n",
       "      <td>0.039666</td>\n",
       "      <td>0.030566</td>\n",
       "      <td>0.033093</td>\n",
       "      <td>0.029715</td>\n",
       "      <td>0.048888</td>\n",
       "      <td>0.045024</td>\n",
       "      <td>0.027695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039898</td>\n",
       "      <td>0.036126</td>\n",
       "      <td>0.046472</td>\n",
       "      <td>0.051336</td>\n",
       "      <td>0.035612</td>\n",
       "      <td>0.022452</td>\n",
       "      <td>0.031476</td>\n",
       "      <td>0.024603</td>\n",
       "      <td>0.034499</td>\n",
       "      <td>0.037122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010666</td>\n",
       "      <td>0.061128</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.061474</td>\n",
       "      <td>0.016788</td>\n",
       "      <td>0.028634</td>\n",
       "      <td>0.026782</td>\n",
       "      <td>0.047735</td>\n",
       "      <td>0.044757</td>\n",
       "      <td>0.029954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016182</td>\n",
       "      <td>0.048841</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>0.037181</td>\n",
       "      <td>0.025678</td>\n",
       "      <td>0.025295</td>\n",
       "      <td>0.022914</td>\n",
       "      <td>0.040654</td>\n",
       "      <td>0.039574</td>\n",
       "      <td>0.035131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.027230</td>\n",
       "      <td>0.038022</td>\n",
       "      <td>0.045059</td>\n",
       "      <td>0.039964</td>\n",
       "      <td>0.046902</td>\n",
       "      <td>0.053649</td>\n",
       "      <td>0.048526</td>\n",
       "      <td>0.089966</td>\n",
       "      <td>0.048509</td>\n",
       "      <td>0.033818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038093</td>\n",
       "      <td>0.073704</td>\n",
       "      <td>0.023961</td>\n",
       "      <td>0.028580</td>\n",
       "      <td>0.029607</td>\n",
       "      <td>0.038643</td>\n",
       "      <td>0.034912</td>\n",
       "      <td>0.041547</td>\n",
       "      <td>0.030148</td>\n",
       "      <td>0.044364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007698</td>\n",
       "      <td>0.048766</td>\n",
       "      <td>0.026887</td>\n",
       "      <td>0.038329</td>\n",
       "      <td>0.011744</td>\n",
       "      <td>0.045210</td>\n",
       "      <td>0.044462</td>\n",
       "      <td>0.070499</td>\n",
       "      <td>0.050655</td>\n",
       "      <td>0.037836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040881</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>0.017581</td>\n",
       "      <td>0.030057</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.017892</td>\n",
       "      <td>0.029770</td>\n",
       "      <td>0.063489</td>\n",
       "      <td>0.031421</td>\n",
       "      <td>0.020286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.033827  0.035713  0.030498  0.018938  0.022437  0.022456  0.077555   \n",
       "1  0.011270  0.065442  0.036901  0.039666  0.030566  0.033093  0.029715   \n",
       "2  0.010666  0.061128  0.029933  0.061474  0.016788  0.028634  0.026782   \n",
       "3  0.027230  0.038022  0.045059  0.039964  0.046902  0.053649  0.048526   \n",
       "4  0.007698  0.048766  0.026887  0.038329  0.011744  0.045210  0.044462   \n",
       "\n",
       "         7         8         9   ...        90        91        92        93  \\\n",
       "0  0.051392  0.013825  0.056622  ...  0.046860  0.058236  0.041051  0.047852   \n",
       "1  0.048888  0.045024  0.027695  ...  0.039898  0.036126  0.046472  0.051336   \n",
       "2  0.047735  0.044757  0.029954  ...  0.016182  0.048841  0.028997  0.037181   \n",
       "3  0.089966  0.048509  0.033818  ...  0.038093  0.073704  0.023961  0.028580   \n",
       "4  0.070499  0.050655  0.037836  ...  0.040881  0.033386  0.017581  0.030057   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0  0.024786  0.018312  0.036336  0.060646  0.023192  0.043599  \n",
       "1  0.035612  0.022452  0.031476  0.024603  0.034499  0.037122  \n",
       "2  0.025678  0.025295  0.022914  0.040654  0.039574  0.035131  \n",
       "3  0.029607  0.038643  0.034912  0.041547  0.030148  0.044364  \n",
       "4  0.020542  0.017892  0.029770  0.063489  0.031421  0.020286  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oxford5k[\"vector norm\"] = {\"test\" : {}, \"train\": {}}\n",
    "oxford5k[\"feature norm\"] = {\"test\" : {}, \"train\": {}}\n",
    "\n",
    "for bovw_size in oxford5k[\"raw features\"][\"train\"].keys():\n",
    "    full_df = pd.concat([oxford5k[\"raw features\"][\"train\"][bovw_size],\n",
    "                         oxford5k[\"raw features\"][\"test\"][bovw_size]],\n",
    "                         ignore_index=True)\n",
    "\n",
    "    feature_norm = sklearn.preprocessing.normalize(full_df, axis=0)\n",
    "    oxford5k[\"feature norm\"][\"train\"][bovw_size] = pd.DataFrame(feature_norm[:567,:])\n",
    "    oxford5k[\"feature norm\"][\"test\"][bovw_size] = pd.DataFrame(feature_norm[567:,:])\n",
    "\n",
    "    vector_norm = sklearn.preprocessing.normalize(full_df, axis=1)\n",
    "    oxford5k[\"vector norm\"][\"train\"][bovw_size] = pd.DataFrame(vector_norm[:567,:])\n",
    "    oxford5k[\"vector norm\"][\"test\"][bovw_size] = pd.DataFrame(vector_norm[567:,:])\n",
    "\n",
    "oxford5k[\"feature norm\"][\"test\"][\"bovw files for 100 Words\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## actual correct way to do transformations, currently not working?\n",
    "\n",
    "# # normalising the columns\n",
    "# train_feature_norm = {}\n",
    "# test_feature_norm = {}\n",
    "\n",
    "# # normalising the rows\n",
    "# train_vector_norm = {}\n",
    "# test_vector_norm = {}\n",
    "\n",
    "# for bovw_size in train_features.keys():\n",
    "#     train = train_features[bovw_size]\n",
    "#     test = test_features[bovw_size]\n",
    "\n",
    "#     row_normaliser = sklearn.preprocessing.Normalizer().fit(train)\n",
    "#     train_vector_norm[bovw_size] = row_normaliser.transform(train)\n",
    "#     test_vector_norm[bovw_size] = row_normaliser.transform(test)\n",
    "\n",
    "#     # rotate dataframes to normalise by column\n",
    "#     temp_train = train.T\n",
    "#     temp_test = test.T\n",
    "#     column_normaliser = sklearn.preprocessing.Normalizer().fit(temp_train)\n",
    "#     train_feature_norm[bovw_size] = column_normaliser.transform(temp_train).T\n",
    "#     test_feature_norm[bovw_size] = column_normaliser.transform(temp_test).T\n",
    "\n",
    "# train_feature_norm([\"bovw files for 10 Words\"]) == sklearn.preprocessing.normalize(train_features[\"bovw files for 10 Words\"], axis=0)\n",
    "\n",
    "# dat = pd.DataFrame({1:[10,-10, 5], 2:[4,-1, 4]})\n",
    "\n",
    "# def normalise_vector(vector, mean=None, stdev=None):\n",
    "#     if not mean:\n",
    "#         mean = np.mean(vector)\n",
    "#     if not stdev:\n",
    "#         stdev = np.std(vector)\n",
    "#     return((vector-mean)/stdev)\n",
    "\n",
    "# dat.apply(normalise_vector, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "### Sample Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2846602906055916,\n",
       " {5: 0.4763636363636364, 10: 0.3636363636363636, 20: 0.2818181818181818})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample query validated against Sean Oldenburger's method\n",
    "\n",
    "compute_metrics(train_features = oxford5k[\"raw features\"][\"train\"][\"bovw files for 10 Words\"],\n",
    "                test_features = oxford5k[\"raw features\"][\"test\"][\"bovw files for 10 Words\"],\n",
    "                train_names = oxford5k[\"attributes\"][\"train\"][\"names\"],\n",
    "                test_names = oxford5k[\"attributes\"][\"test\"][\"names\"],\n",
    "                query_function=basic_query,\n",
    "                metric_function = sklearn.metrics.pairwise.euclidean_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "### Test all experimental setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_distance_metrics = {\"euclidean\" : sklearn.metrics.pairwise.euclidean_distances,\n",
    "           \"cosine\" : sklearn.metrics.pairwise.cosine_distances,\n",
    "           \"manhattan\" :sklearn.metrics.pairwise.manhattan_distances,\n",
    "           \"nan_euclidean\" : sklearn.metrics.pairwise.nan_euclidean_distances}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclidean raw features bovw files for 10000 Words : 0.5160431683671248\n",
      "euclidean raw features bovw files for 10 Words : 0.2846602906055916\n",
      "euclidean raw features bovw files for 100 Words : 0.3836962289612868\n",
      "euclidean raw features bovw files for 20000 Words : 0.5679781603977279\n",
      "euclidean raw features bovw files for 100000 Words : 0.6682471101801445\n",
      "euclidean raw features bovw files for 1000 Words : 0.4096111116568395\n",
      "euclidean raw features bovw files for 50000 Words : 0.6198841133826765\n",
      "euclidean vector norm bovw files for 10000 Words : 0.5160431683671248\n",
      "euclidean vector norm bovw files for 10 Words : 0.2846602906055916\n",
      "euclidean vector norm bovw files for 100 Words : 0.3836962289612868\n",
      "euclidean vector norm bovw files for 20000 Words : 0.5679781603977279\n",
      "euclidean vector norm bovw files for 100000 Words : 0.6682471101801445\n",
      "euclidean vector norm bovw files for 1000 Words : 0.4096111116568395\n",
      "euclidean vector norm bovw files for 50000 Words : 0.6198841133826765\n",
      "euclidean feature norm bovw files for 10000 Words : 0.23198734193283227\n",
      "euclidean feature norm bovw files for 10 Words : 0.29116322636257014\n",
      "euclidean feature norm bovw files for 100 Words : 0.4131998203589281\n",
      "euclidean feature norm bovw files for 20000 Words : 0.21273596725035435\n",
      "euclidean feature norm bovw files for 100000 Words : 0.20411431859694523\n",
      "euclidean feature norm bovw files for 1000 Words : 0.2842654768447032\n",
      "euclidean feature norm bovw files for 50000 Words : 0.21847677783972957\n",
      "cosine raw features bovw files for 10000 Words : 0.5160431683671248\n",
      "cosine raw features bovw files for 10 Words : 0.2846602906055916\n",
      "cosine raw features bovw files for 100 Words : 0.3836962289612868\n",
      "cosine raw features bovw files for 20000 Words : 0.5679781603977279\n",
      "cosine raw features bovw files for 100000 Words : 0.6682471101801445\n",
      "cosine raw features bovw files for 1000 Words : 0.4096111116568395\n",
      "cosine raw features bovw files for 50000 Words : 0.6198841133826765\n",
      "cosine vector norm bovw files for 10000 Words : 0.5160431683671248\n",
      "cosine vector norm bovw files for 10 Words : 0.2846602906055916\n",
      "cosine vector norm bovw files for 100 Words : 0.3836962289612868\n",
      "cosine vector norm bovw files for 20000 Words : 0.5679781603977279\n",
      "cosine vector norm bovw files for 100000 Words : 0.6682471101801445\n",
      "cosine vector norm bovw files for 1000 Words : 0.4096111116568395\n",
      "cosine vector norm bovw files for 50000 Words : 0.6198841133826765\n",
      "cosine feature norm bovw files for 10000 Words : 0.5068441560887526\n",
      "cosine feature norm bovw files for 10 Words : 0.2929153191889162\n",
      "cosine feature norm bovw files for 100 Words : 0.40530701608671466\n",
      "cosine feature norm bovw files for 20000 Words : 0.5590395183069514\n",
      "cosine feature norm bovw files for 100000 Words : 0.6997034939349722\n",
      "cosine feature norm bovw files for 1000 Words : 0.3808193045999128\n",
      "cosine feature norm bovw files for 50000 Words : 0.6488484613073342\n",
      "manhattan raw features bovw files for 10000 Words : 0.31391666271372104\n",
      "manhattan raw features bovw files for 10 Words : 0.28016695234740613\n",
      "manhattan raw features bovw files for 100 Words : 0.41987751669050827\n",
      "manhattan raw features bovw files for 20000 Words : 0.28554885938607827\n",
      "manhattan raw features bovw files for 100000 Words : 0.23436251972209132\n",
      "manhattan raw features bovw files for 1000 Words : 0.45417839082446715\n",
      "manhattan raw features bovw files for 50000 Words : 0.2627765163004305\n",
      "manhattan vector norm bovw files for 10000 Words : 0.31391666271372104\n",
      "manhattan vector norm bovw files for 10 Words : 0.28016695234740613\n",
      "manhattan vector norm bovw files for 100 Words : 0.41987751669050827\n",
      "manhattan vector norm bovw files for 20000 Words : 0.28554885938607827\n",
      "manhattan vector norm bovw files for 100000 Words : 0.23436251972209132\n",
      "manhattan vector norm bovw files for 1000 Words : 0.45417839082446715\n",
      "manhattan vector norm bovw files for 50000 Words : 0.2627765163004305\n",
      "manhattan feature norm bovw files for 10000 Words : 0.2445966277346388\n",
      "manhattan feature norm bovw files for 10 Words : 0.2856920250417519\n",
      "manhattan feature norm bovw files for 100 Words : 0.4237357636131409\n",
      "manhattan feature norm bovw files for 20000 Words : 0.225294734176115\n",
      "manhattan feature norm bovw files for 100000 Words : 0.2051155131387044\n",
      "manhattan feature norm bovw files for 1000 Words : 0.3482093541018759\n",
      "manhattan feature norm bovw files for 50000 Words : 0.21399793083998986\n",
      "nan_euclidean raw features bovw files for 10000 Words : 0.5160431683671248\n",
      "nan_euclidean raw features bovw files for 10 Words : 0.2846602906055916\n",
      "nan_euclidean raw features bovw files for 100 Words : 0.3836962289612868\n",
      "nan_euclidean raw features bovw files for 20000 Words : 0.5679781603977279\n",
      "nan_euclidean raw features bovw files for 100000 Words : 0.6682471101801445\n",
      "nan_euclidean raw features bovw files for 1000 Words : 0.4096111116568395\n",
      "nan_euclidean raw features bovw files for 50000 Words : 0.6198841133826765\n",
      "nan_euclidean vector norm bovw files for 10000 Words : 0.5160431683671248\n",
      "nan_euclidean vector norm bovw files for 10 Words : 0.2846602906055916\n",
      "nan_euclidean vector norm bovw files for 100 Words : 0.3836962289612868\n",
      "nan_euclidean vector norm bovw files for 20000 Words : 0.5679781603977279\n",
      "nan_euclidean vector norm bovw files for 100000 Words : 0.6682471101801445\n",
      "nan_euclidean vector norm bovw files for 1000 Words : 0.4096111116568395\n",
      "nan_euclidean vector norm bovw files for 50000 Words : 0.6198841133826765\n",
      "nan_euclidean feature norm bovw files for 10000 Words : 0.23198734193283227\n",
      "nan_euclidean feature norm bovw files for 10 Words : 0.29116322636257014\n",
      "nan_euclidean feature norm bovw files for 100 Words : 0.4131998203589281\n",
      "nan_euclidean feature norm bovw files for 20000 Words : 0.21273596725035435\n",
      "nan_euclidean feature norm bovw files for 100000 Words : 0.20411431859694523\n",
      "nan_euclidean feature norm bovw files for 1000 Words : 0.2842654768447032\n",
      "nan_euclidean feature norm bovw files for 50000 Words : 0.21847677783972957\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_metrics</th>\n",
       "      <th>transformations</th>\n",
       "      <th>bovw_sizes</th>\n",
       "      <th>mean_aps</th>\n",
       "      <th>precision at 5</th>\n",
       "      <th>precision at 10</th>\n",
       "      <th>precision at 20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>euclidean</td>\n",
       "      <td>raw features</td>\n",
       "      <td>bovw files for 10000 Words</td>\n",
       "      <td>0.516043</td>\n",
       "      <td>0.301818</td>\n",
       "      <td>0.238182</td>\n",
       "      <td>0.184545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>euclidean</td>\n",
       "      <td>raw features</td>\n",
       "      <td>bovw files for 10 Words</td>\n",
       "      <td>0.284660</td>\n",
       "      <td>0.301818</td>\n",
       "      <td>0.238182</td>\n",
       "      <td>0.184545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>euclidean</td>\n",
       "      <td>raw features</td>\n",
       "      <td>bovw files for 100 Words</td>\n",
       "      <td>0.383696</td>\n",
       "      <td>0.301818</td>\n",
       "      <td>0.238182</td>\n",
       "      <td>0.184545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>euclidean</td>\n",
       "      <td>raw features</td>\n",
       "      <td>bovw files for 20000 Words</td>\n",
       "      <td>0.567978</td>\n",
       "      <td>0.301818</td>\n",
       "      <td>0.238182</td>\n",
       "      <td>0.184545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>euclidean</td>\n",
       "      <td>raw features</td>\n",
       "      <td>bovw files for 100000 Words</td>\n",
       "      <td>0.668247</td>\n",
       "      <td>0.301818</td>\n",
       "      <td>0.238182</td>\n",
       "      <td>0.184545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>nan_euclidean</td>\n",
       "      <td>feature norm</td>\n",
       "      <td>bovw files for 100 Words</td>\n",
       "      <td>0.413200</td>\n",
       "      <td>0.301818</td>\n",
       "      <td>0.238182</td>\n",
       "      <td>0.184545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>nan_euclidean</td>\n",
       "      <td>feature norm</td>\n",
       "      <td>bovw files for 20000 Words</td>\n",
       "      <td>0.212736</td>\n",
       "      <td>0.301818</td>\n",
       "      <td>0.238182</td>\n",
       "      <td>0.184545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>nan_euclidean</td>\n",
       "      <td>feature norm</td>\n",
       "      <td>bovw files for 100000 Words</td>\n",
       "      <td>0.204114</td>\n",
       "      <td>0.301818</td>\n",
       "      <td>0.238182</td>\n",
       "      <td>0.184545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>nan_euclidean</td>\n",
       "      <td>feature norm</td>\n",
       "      <td>bovw files for 1000 Words</td>\n",
       "      <td>0.284265</td>\n",
       "      <td>0.301818</td>\n",
       "      <td>0.238182</td>\n",
       "      <td>0.184545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>nan_euclidean</td>\n",
       "      <td>feature norm</td>\n",
       "      <td>bovw files for 50000 Words</td>\n",
       "      <td>0.218477</td>\n",
       "      <td>0.301818</td>\n",
       "      <td>0.238182</td>\n",
       "      <td>0.184545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   distance_metrics transformations                   bovw_sizes  mean_aps  \\\n",
       "0         euclidean    raw features   bovw files for 10000 Words  0.516043   \n",
       "1         euclidean    raw features      bovw files for 10 Words  0.284660   \n",
       "2         euclidean    raw features     bovw files for 100 Words  0.383696   \n",
       "3         euclidean    raw features   bovw files for 20000 Words  0.567978   \n",
       "4         euclidean    raw features  bovw files for 100000 Words  0.668247   \n",
       "..              ...             ...                          ...       ...   \n",
       "79    nan_euclidean    feature norm     bovw files for 100 Words  0.413200   \n",
       "80    nan_euclidean    feature norm   bovw files for 20000 Words  0.212736   \n",
       "81    nan_euclidean    feature norm  bovw files for 100000 Words  0.204114   \n",
       "82    nan_euclidean    feature norm    bovw files for 1000 Words  0.284265   \n",
       "83    nan_euclidean    feature norm   bovw files for 50000 Words  0.218477   \n",
       "\n",
       "    precision at 5  precision at 10  precision at 20  \n",
       "0         0.301818         0.238182         0.184545  \n",
       "1         0.301818         0.238182         0.184545  \n",
       "2         0.301818         0.238182         0.184545  \n",
       "3         0.301818         0.238182         0.184545  \n",
       "4         0.301818         0.238182         0.184545  \n",
       "..             ...              ...              ...  \n",
       "79        0.301818         0.238182         0.184545  \n",
       "80        0.301818         0.238182         0.184545  \n",
       "81        0.301818         0.238182         0.184545  \n",
       "82        0.301818         0.238182         0.184545  \n",
       "83        0.301818         0.238182         0.184545  \n",
       "\n",
       "[84 rows x 7 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {\"distance_metrics\" : [], 'transformations' : [], 'bovw_sizes' : [], 'mean_aps' : []}\n",
    "k_vals = [5, 10, 20]\n",
    "for k in k_vals:\n",
    "    results[\"precision at {}\".format(k)] = []\n",
    "\n",
    "for (metric_name, metric_function) in test_distance_metrics.items():\n",
    "    for  transformation in [\"raw features\", \"vector norm\", \"feature norm\"]:\n",
    "        train = oxford5k[transformation][\"train\"]\n",
    "        test = oxford5k[transformation][\"test\"]\n",
    "        \n",
    "        for bovw_size in train.keys():\n",
    "            (mean_ap, p_at_k) = compute_metrics(train_features = train[bovw_size],\n",
    "                                                test_features = test[bovw_size],\n",
    "                                                train_names = oxford5k[\"attributes\"][\"train\"][\"names\"],\n",
    "                                                test_names = oxford5k[\"attributes\"][\"test\"][\"names\"],\n",
    "                                                query_function=basic_query,\n",
    "                                                metric_function = metric_function,\n",
    "                                                k_values=k_vals)\n",
    "            \n",
    "            print(\"{} {} {} : {}\".format(metric_name, transformation, bovw_size, mean_ap))\n",
    "            results['distance_metrics'].append(metric_name)\n",
    "            results['transformations'].append(transformation)\n",
    "            results['bovw_sizes'].append(bovw_size)\n",
    "            results['mean_aps'].append(mean_ap)\n",
    "            for (k, p_at_k) in p_at_k.items():\n",
    "                results[\"precision at {}\".format(k)] = p_at_k\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "### Visualising results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e0edef247045f2f5f35ac9d6435770b0c68a1ddd7eb34b4959830e587ac51e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
